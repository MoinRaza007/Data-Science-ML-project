# -*- coding: utf-8 -*-
"""Employee_Turnover_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kc12vfU3nrK0R74el_cGGWPUJC8xxBg_
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from imblearn.over_sampling import SMOTE

# Load the Excel file into a DataFrame
df = pd.read_csv('/content/WA_Fn-UseC_-HR-Employee-Attrition.csv')

df.shape

data=pd.DataFrame(df)

df.head(10)

df.tail(10)

# Check for missing values
print(df.isnull().sum())

# Drop rows with missing values
df = df.dropna()

# Check for duplicate rows
print(df.duplicated().sum())

# Drop duplicate rows
df = df.drop_duplicates()

df.info('Age')

df.describe()

print(df.head())

from matplotlib import pyplot as plt

ColsBox = df.select_dtypes('int64')
for col in ColsBox.columns:
    plt.figure(figsize=(10,6))
    plt.title('box plot of '+col)
    sns.boxplot(df[col])
    plt.show()

def remove_outliers(df):
    for col in df.select_dtypes(include='number').columns:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 2 * iqr
        upper_bound = q3 + 2 * iqr
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

df = remove_outliers(df)

ColsBox = df.select_dtypes('int64')
for col in ColsBox.columns:
    plt.figure(figsize=(10,6))
    plt.title('box plot of '+col)
    sns.boxplot(df[col])
    plt.show()

plt.figure(figsize = (10,6))
sns.histplot(df['Age'] , kde = True , color = 'orange')
plt.title('Histogram and Kde plot of Age')
plt.show()

plt.figure(figsize = (10,6))
sns.regplot(x = df['Age'] , y = df['MonthlyIncome'])
plt.show()

df['Gender'].value_counts().plot(kind = 'bar')
plt.show()

df.groupby('Gender').Attrition.value_counts().plot( kind = 'bar')
plt.show()

df['MaritalStatus'].value_counts().plot(kind = 'pie' , autopct = "%1.1f%%")
plt.show()

df.groupby('JobSatisfaction').Attrition.value_counts().sort_values(ascending = False).plot(kind = "bar")
plt.show()

df.groupby('Attrition')['MonthlyIncome'].mean().plot(kind = "bar")
plt.show()

df.groupby('EnvironmentSatisfaction').Attrition.value_counts().sort_values(ascending = False).plot(kind = "bar")
plt.show()

df.groupby('Attrition')['NumCompaniesWorked'].mean().plot(kind = "bar")
plt.show()

df.groupby('WorkLifeBalance').Attrition.value_counts().sort_values(ascending = False).plot(kind = "bar")
plt.show()

print(df.groupby('Attrition')['Age'].mean())

df.groupby('Attrition')['BusinessTravel'].value_counts().plot(kind ='bar' ,color = 'orange')
plt.show()

df['Over18'].value_counts()

df.drop(columns=['Over18'] , inplace = True)

df['BusinessTravel'].value_counts()

df['BusinessTravel'] = df['BusinessTravel'].replace({'Travel_Rarely':0 ,'Travel_Frequently':1 , 'Non-Travel':3})

df['BusinessTravel'] = df['BusinessTravel'].astype('int64')

df['Department'] = df['Department'].replace({'Research & Development':0 ,'Sales':1 , 'Human Resources':3})
df['Department'] = df['Department'].astype('int64')

encoder = LabelEncoder()

df['EducationField'] = encoder.fit_transform(df['EducationField'])

df['Gender'] = encoder.fit_transform(df['Gender'])

df['JobRole'] = encoder.fit_transform(df['JobRole'])

df['MaritalStatus'] = encoder.fit_transform(df['MaritalStatus'])

df['OverTime'] = encoder.fit_transform(df['OverTime'])

df.info()

df['Attrition'] = df['Attrition'].map({'Yes': 1, 'No': 0})

plt.figure(figsize=(12,8))
sns.heatmap(df.corr(),cmap = 'coolwarm')
plt.show()

scaler = MinMaxScaler()

df[df.columns] = scaler.fit_transform(df[df.columns])

df.head()

X = df.drop('Attrition', axis=1)
y = df['Attrition']

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

classifiers = {
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Logistic Regression": LogisticRegression(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "Naive Bayes": GaussianNB()
}

for clf_name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    f2 = fbeta_score(y_test, y_pred, beta=2)
    print(f"Classifier: {clf_name}")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1 Score: {f1:.2f}")
    print(f"F2 Score: {f2:.2f}")
    print("---------------------------")

X = df.drop('Attrition', axis=1)
y = df['Attrition']

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=60)

rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
acc=accuracy*100
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
f2 = fbeta_score(y_test, y_pred, beta=2)

print("Random Forest")
print(f"Accuracy: {accuracy:.4f}")
print(f"Accuracy: {acc:.2f} %")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")
print(f"F2 Score: {f2:.2f}")

