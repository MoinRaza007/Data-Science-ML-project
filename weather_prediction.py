# -*- coding: utf-8 -*-
"""Weather_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HOjAmB5wvzZIvpEbMNUhezky0Pj0P7gL
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE

df= pd.read_csv("/content/temperature.csv")

df.head()

df.tail()

df.shape

data=pd.DataFrame(df)

# Checking for missing values if any
missing_values = df.isnull().sum()
print(missing_values)

# Forward-filling all missing values
df.fillna(method='ffill', inplace=True)

df.info()

df.describe()

print(df.head())

# Checking for missing values if any
missing_values = df.isnull().sum()
print(missing_values)

# Forward and backward filling all missing values again
df.fillna(method='ffill', inplace=True)
df.fillna(method='bfill', inplace=True)

# Ensure no NaN values remain
assert not df.isnull().any().any(), "There are still NaN values in the DataFrame."

ColsBox = df.select_dtypes('int64')
for col in ColsBox.columns:
    plt.figure(figsize=(10,6))
    plt.title('box plot of '+col)
    sns.boxplot(df[col])
    plt.show()

# Plotting temperature for some city San Fransisco
plt.figure(figsize=(12, 6))
plt.plot(df['San Francisco'], label='Temperature ( San Francisco)')
plt.title('Hourly Temperature in  San Francisco')
plt.xlabel('Date')
plt.ylabel('Temperature (Kelvin)')
plt.legend()
plt.show()

# Plotting temperature for some city San Fransisco
plt.figure(figsize=(12, 6))
plt.plot(df['Boston'], label='Temperature (Boston)')
plt.title('Hourly Temperature in Boston')
plt.xlabel('Date')
plt.ylabel('Temperature (Kelvin)')
plt.legend()
plt.show()

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

target_city = 'New York'
temperature = df[[target_city]].values

# Normalizing features
scaler = MinMaxScaler(feature_range=(0, 1))
temperature_scaled = scaler.fit_transform(temperature)

# Splitting the data into training and testing sets
train_size = int(len(temperature_scaled) * 0.8)
test_size = len(temperature_scaled) - train_size
train, test = temperature_scaled[0:train_size,:], temperature_scaled[train_size:len(temperature_scaled),:]

# Shaping data to be suitable for LSTM

# Converting an array of values into a dataset matrix for LSTM
def create_dataset(dataset, look_back=1):
    X, Y = [], []
    for i in range(len(dataset)-look_back-1):
        a = dataset[i:(i+look_back), 0]
        X.append(a)
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

look_back = 24
X_train, Y_train = create_dataset(train, look_back)
X_test, Y_test = create_dataset(test, look_back)

# Input will be: [samples, time steps, features]
X_train = np.reshape(X_train, (X_train.shape[0], look_back, 1))
X_test = np.reshape(X_test, (X_test.shape[0], look_back, 1))

import torch
import torch.nn as nn

class LSTMModel(nn.Module):
    def __init__(self, input_size, hidden_layer_size, num_layers, output_size):
        super().__init__()
        self.hidden_layer_size = hidden_layer_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers, batch_first=True)
        self.linear = nn.Linear(hidden_layer_size, output_size)

    def forward(self, input_seq):
        lstm_out, _ = self.lstm(input_seq)
        lstm_out_last_step = lstm_out[:, -1, :]
        predictions = self.linear(lstm_out_last_step)
        return predictions

input_size = 1
hidden_layer_size = 50
num_layers = 1
output_size = 1

model = LSTMModel(input_size, hidden_layer_size, num_layers, output_size)

# Defining the loss function and the optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

from torch.utils.data import TensorDataset, DataLoader

X_train_tensor = torch.Tensor(X_train)
Y_train_tensor = torch.Tensor(Y_train).view(-1, 1)

train_data = TensorDataset(X_train_tensor, Y_train_tensor)
train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)

# Training
epochs = 100
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

model.train()
for i in range(epochs):
    for seq, labels in train_loader:
        seq, labels = seq.to(device), labels.to(device)
        optimizer.zero_grad()
        y_pred = model(seq)
        single_loss = criterion(y_pred, labels)
        single_loss.backward()

        # Gradient clipping
        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)

        optimizer.step()

    if i % 25 == 0:
        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')

print(f'epoch: {epochs:3} loss: {single_loss.item():10.10f}')

# Evaluation
X_test_tensor = torch.Tensor(X_test)
Y_test_tensor = torch.Tensor(Y_test).view(-1, 1)

# Create DataLoader for testing data
test_data = TensorDataset(X_test_tensor, Y_test_tensor)
test_loader = DataLoader(test_data, batch_size=1, shuffle=False, drop_last=True)

from sklearn.metrics import mean_absolute_error, mean_squared_error

actuals = np.array([label.item() for label in Y_test_tensor])
predictions = []

model.eval()
with torch.no_grad():
    for seq, labels in test_loader:
        seq = seq.to(device)
        y_pred = model(seq).cpu().numpy()
        predictions.append(y_pred[0][0])

predictions = np.array(predictions)

# Calculate descriptive statistics
mae = mean_absolute_error(actuals, predictions)
rmse = np.sqrt(mean_squared_error(actuals, predictions))
mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100

print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Percentage Error (MAPE): {mape:.2f}%")

# Visualize predictions vs actuals
plt.figure(figsize=(15, 5))
plt.plot(actuals, label='Actual Values')
plt.plot(predictions, label='Predictions', alpha=0.7)
plt.title('Actual vs Predicted Values')
plt.xlabel('Time Steps')
plt.ylabel('Normalized Temperature')
plt.legend()
plt.show()

# Plot residuals
residuals = actuals - predictions
plt.figure(figsize=(15, 5))
plt.plot(residuals, label='Residuals')
plt.title('Residuals of Predictions')
plt.xlabel('Time Steps')
plt.ylabel('Error')
plt.legend()
plt.show()

# Histogram of the residuals
plt.figure(figsize=(8, 5))
plt.hist(residuals, bins=50, alpha=0.7, color='blue', edgecolor='black')
plt.title('Distribution of Residuals')
plt.xlabel('Error')
plt.ylabel('Frequency')
plt.show()

